{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Выделим два класса для нашей модели\n",
    "iris.target_names[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделим значения двух классов\n",
    "X = iris.data[50:]\n",
    "y = iris.target[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Значениям классов присвоим 0 и 1\n",
    "y = y - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В матрицу значений признаков добавим фиктивный признак (свободный член модели логистической регрессии)\n",
    "X = np.c_[np.ones((100,1)),X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 7. , 3.2, 4.7, 1.4],\n",
       "       [1. , 6.4, 3.2, 4.5, 1.5],\n",
       "       [1. , 6.9, 3.1, 4.9, 1.5],\n",
       "       [1. , 5.5, 2.3, 4. , 1.3],\n",
       "       [1. , 6.5, 2.8, 4.6, 1.5]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель логистической регрессии: \n",
    "$L_\\theta = \\theta_0 + \\theta_1X_1 + \\theta_2X_2 + \\theta_3X_3 + \\theta_4X_4$  \n",
    "  \n",
    "  В матричном виде: $L = X\\theta$,  где $X = {\\begin{pmatrix} 1 & x_{11} & ... & x_{14}\\\\  1 & x_{21} & ... & x_{24}\\\\ .&.&.&.\\\\1 & x_{41} & ... & x_{44}\\end{pmatrix}}$,  $\\theta = {\\begin{pmatrix}\\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\\\ \\theta_3 \\\\ \\theta_4\\end{pmatrix}}$\n",
    "  \n",
    "Также можно записать в виде: \n",
    "$$L(x_i) = \\theta_0 + \\theta_1x_{i1} + \\theta_2x_{i2} + \\theta_3x_{i3} + \\theta_4x_{i4}$$  \n",
    "Вероятности вычисляются по формуле: $$h_\\theta(x_i) = \\frac{1}{1 + e^{-L_\\theta(x_i)}}$$  \n",
    "Для двух классов функция ошибки имеет вид:  \n",
    "$$J = -\\sum_{i=1}^{100}\\big(y_i\\ln(h_\\theta(x_i)) + (1 - y_i)\\ln(h_\\theta(x_i))\\big)$$  \n",
    "$y_i$ принимают значения: 0 или 1.  \n",
    "  \n",
    "Подставив выражение для $h_\\theta(x_i)$ получим:  \n",
    "$$J = \\sum_{i=1}^{100}\\big(\\ln(1 + e^{-L(x_i)}) + (1 - y_i)L(x_i)\\big)$$\n",
    "  \n",
    "Градиент функции ошибки от параметра $\\theta$ - это частные производные по значениям параметра $\\theta_j$:  \n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\sum_{i=1}^{100}\\big(1 - y_i + \\frac{e^{-L(x_i)}}{1 + e^{-L(x_i)}}\\big)\\frac{\\partial L(x_i)}{\\partial \\theta_j}$$  \n",
    "\n",
    "В векторной форме:  \n",
    "$$grad = {\\nabla_{\\theta}J(\\theta)} = {\\begin{pmatrix}J'_{\\theta_0}\\\\J'_{\\theta_1}\\\\J'_{\\theta_2}\\\\J'_{\\theta_3}\\\\J'_{\\theta_4} \\end{pmatrix}} = {\\begin{pmatrix}\\frac{1}{1 + e^{-X\\theta}} - y\\end{pmatrix}}X$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##   <p style=\"text-align: center;\">Метод градиентного спуска</p>\n",
    "\n",
    "\n",
    "$$\\Delta\\theta = -\\eta\\nabla_{\\theta}J(\\theta)$$  \n",
    "\n",
    "$$\\theta = \\theta + \\Delta\\theta = \\theta - \\eta\\nabla_{\\theta}J(\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_log_reg(eta, X, y, theta, iterations):\n",
    "    \n",
    "    \"\"\"Функция реализует метод градиентного спуска для логистической регрессии\"\"\"\n",
    "    \n",
    "    cost_gd = np.array([])\n",
    "    theta_gd = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "                      \n",
    "        gradient = np.dot(1 / (1 + np.exp(- np.dot(X, theta))) - y, X)\n",
    "        \n",
    "        theta = theta - eta * gradient\n",
    "        \n",
    "        J = np.sum((1 - y)*np.dot(X, theta) + np.log(1 + np.exp(- np.dot(X, theta)))) # Loss function for \n",
    "                                                                                      # Logistic Regression\n",
    "                      \n",
    "        cost_gd = np.append(cost_gd, J)\n",
    "        theta_gd.append(theta.T)\n",
    "                     \n",
    "    return cost_gd, theta_gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_score(theta):\n",
    "    \n",
    "    \"\"\"По параметру theta вычисляет вероятности для значений признаков, округлением присваивает класс (если \n",
    "    вероятность больше 0.5, то присваивает класс 1, в противном случае класс 0) и выводит количество совпавших  \n",
    "    с целевым признаком классов, делённое на 100.\"\"\"\n",
    "    \n",
    "    pred = np.zeros(100)\n",
    "    for i in range(100):\n",
    "        pred[i] = np.round(1 / (1 + np.exp(- np.dot(X[i], theta))))\n",
    "    return 1 - sum(np.abs(y - pred)) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = np.array([0.1, 0.1, 0.1, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_gd, theta_gd = gradient_log_reg(0.01, X, y, theta0, 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.07585666, 6.07585108, 6.0758455 , 6.07583992, 6.07583434,\n",
       "       6.07582876, 6.07582318, 6.0758176 , 6.07581202, 6.07580644])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_gd[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-31.1008369 ,  -2.70947193,  -5.74402572,   7.98847701,\n",
       "        14.96589469])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_gd[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_score(theta_gd[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На 40000-й итерации для параметров: $\\theta_0=(0.1,\\ \\ 0.1,\\ \\ 0.1,\\ \\ 0.1)$ и $\\eta=0.01$ значение функции потери $J=6.07580644$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <p style=\"text-align: center;\">Ускоренные градиенты Нестерова (NAG)</p>\n",
    "\n",
    "  \n",
    "\n",
    "$$\\Delta_{p+1} = \\gamma\\Delta_p + \\eta\\nabla{J(\\theta_p - \\gamma\\Delta_p)}$$  \n",
    "\n",
    "$$\\theta_{p+1} = \\theta_p - \\Delta_{p+1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAG(eta, gamma, X, y, theta, iterations):\n",
    "    \n",
    "    \"\"\"Функция реализует метод ускоренных градиентов Нестерова для логистической регрессии\"\"\"\n",
    "    \n",
    "    delta = np.zeros(5)\n",
    "    cost_nag = np.array([])\n",
    "    theta_nag = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        gradient = np.dot(1 / (1 + np.exp(- np.dot(X, theta - gamma * delta))) - y, X)\n",
    "        \n",
    "        delta = gamma * delta + eta * gradient\n",
    "        theta = theta - delta\n",
    "        \n",
    "        J = np.sum((1 - y)*np.dot(X, theta) + np.log(1 + np.exp(- np.dot(X, theta)))) # Loss function for \n",
    "                                                                                      # Logistic Regression\n",
    "        \n",
    "        cost_nag = np.append(cost_nag, J)\n",
    "        theta_nag.append(theta.T)\n",
    "        \n",
    "    return cost_nag, np.array(theta_nag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_nag, theta_nag = NAG(0.01, 0.9, X, y, theta0, 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.94927403, 5.94927403, 5.94927403, 5.94927403, 5.94927403])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Значения функции потери при последних 5 итераций\n",
    "cost_nag[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-42.60889494,  -2.4655599 ,  -6.67835358,   9.42539695,\n",
       "        18.27758572])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Значения θ, полученные при последней итерации\n",
    "theta_nag[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Предсказание для последнего значения θ\n",
    "prediction_score(theta_nag[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При оптимизации методом Нестерова сходимость быстрее. При увеличении параметра $\\gamma$ на 0.05 сходимость ускоряется к значению $J\\approx5.9492$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <p style=\"text-align: center;\">Метод RMSprop</p>\n",
    "\n",
    "$$\\nabla{J(\\theta_p)} = {\\begin{pmatrix}g_p^{(1)}, & g_p^{(2)}, & g_p^{(3)}, & g_p^{(4)}\\end{pmatrix}}$$  \n",
    "  \n",
    "  Экспоненциально взвешенное скользящее среднее градиентов:  \n",
    "  \n",
    "$$EG_{p+1}^{(i)} = \\gamma{EG_p^{(i)}} + (1 - \\gamma)\\big(g_p^{(i)}\\big)^2$$  \n",
    "$$\\theta_{p+1} = \\theta_p - \\frac{\\eta}{\\sqrt{EG_{p+1}+\\epsilon}}\\nabla{J(\\theta_p)},\\           \\ где\\     \\ \\epsilon\\approx 10^{-8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSprop(eta, gamma, X, y, theta, iterations):\n",
    "    \n",
    "    \"\"\"Функция реализует метод RMSprop для логистической регрессии\"\"\"\n",
    "    \n",
    "    eps = 0.00000001\n",
    "    EG = (np.dot(1 / (1 + np.exp(- np.dot(X, theta))) - y, X))**2\n",
    "    cost_rms = np.array([])\n",
    "    theta_rms = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        gradient = np.dot(1 / (1 + np.exp(- np.dot(X, theta))) - y, X)\n",
    "        \n",
    "        EG = gamma * EG + (1 - gamma) * (gradient**2)\n",
    "        \n",
    "        theta = theta - (eta / np.sqrt(EG + eps)) * gradient\n",
    "                \n",
    "        J = np.sum((1 - y)*np.dot(X, theta) + np.log(1 + np.exp(- np.dot(X, theta)))) # Loss function for \n",
    "                                                                                      # Logistic Regression\n",
    "        \n",
    "        cost_rms = np.append(cost_rms, J)\n",
    "        theta_rms.append(theta.T)\n",
    "        \n",
    "    return cost_rms, np.array(theta_rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_rms, theta_rms = RMSprop(0.01, 0.9, X, y, theta0, 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.95630284, 5.9562139 , 5.95630259, 5.95621365, 5.95630234])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Значения функции потери при последних 5 итерациях\n",
    "cost_rms[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-41.66807294,  -2.47175781,  -6.58652312,   9.30024223,\n",
       "        18.00176945])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Значения θ, полученные при последней итерации\n",
    "theta_rms[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Предсказание для последнего значения θ\n",
    "prediction_score(theta_rms[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для метода **RMSprop** сходимость к минимуму функции потери немного медленне, чем при методе Нестерова."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
